{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>The Normal Equation for OLS Regression</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "import numpy.linalg as npla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Linear Regression</h1>\n",
    "<ul>\n",
    "    <li>How does linear regression find the best model?</li>\n",
    "    <li>Recap:\n",
    "        <ul>\n",
    "            <li>The learner is given a labeled training set ($m \\times n$ matrix $\\v{X}$ and \n",
    "                $m$-dimensional vector $\\v{y}$) and it inserts an extra element $\\v{x}_0^{(i)}$ in each\n",
    "                row $i$, all of which will be 1\n",
    "                $$\\v{X} = \\begin{bmatrix}\n",
    "                    \\v{x}_0^{(1)} = 1 & \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "                    \\v{x}_0^{(2)} = 1 & \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "                    \\vdots            & \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "                    \\v{x}_0^{(m)} = 1 & \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "                          \\end{bmatrix}\\,\\,\\,\\,\n",
    "                   \\v{y} = \\cv{y^{(1)}\\\\ y^{(2)}\\\\ \\vdots\\\\ y^{(m)}}\n",
    "                 $$\n",
    "             </li>\n",
    "             <li>An $n$-dimensional column vector \n",
    "                 $\\v{\\beta} = \\cv{\\v{\\beta}_0\\\\ \\v{\\beta}_1\\\\ \\vdots\\\\ \\v{\\beta}_n}$\n",
    "                 gives us a hypothesis $h_{\\v{\\beta}}$ where\n",
    "                 $$\\hat{y} = h_{\\v{\\beta}}(\\v{x}) = \\v{x}\\v{\\beta}$$\n",
    "             <li>In the case of Ordinary Least Squares (OLS) regression, we must find the vector $\\v{\\beta}$\n",
    "                 that minimizes the following loss function\n",
    "                 $$J(\\v{X}, \\v{y}, \\v{\\beta}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2$$\n",
    "             </li>\n",
    "             <li>In Machine Learning, the variables whose values we are trying to find (in this case, $\\v{\\beta}$)\n",
    "                are called the <b>parameters</b>.\n",
    "             </li>\n",
    "         </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for OLS regression (assumes X contains all 1s in its first column)\n",
    "def J(X, y, beta):\n",
    "    return np.mean((X.dot(beta) - y) ** 2) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>2D Visualization of $J$</h1>\n",
    "<ul>\n",
    "    <li>Let's visualize $J$ using the Cork Property Prices Dataset as the training set.</li>\n",
    "    <li>For a 2D visualization, we'll assume that $\\v{\\beta}_j = 0$ for all $j$ except $j = 1$.\n",
    "        <ul>\n",
    "            <li>In other words, we are pretending that floor area is the only relevant feature.\n",
    "            </li>\n",
    "        </ul>\n",
    "        Then we can plot $J$ on the vertical axis against different values of\n",
    "        $\\v{\\beta}_{1}$ on the horizontal axis.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Get the feature-values (just flarea) and the target values \n",
    "X = df[[\"flarea\"]].values\n",
    "y = df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def show_linear_model_and_loss(beta1):\n",
    "    fig = plt.figure(figsize=(8, 4)) \n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
    "    \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    plt.title(\"Training set\")\n",
    "    plt.xlabel(\"Floor area (sq metres)\")\n",
    "    plt.xlim(-500, 500)\n",
    "    plt.ylabel(\"Price (000 euros)\")\n",
    "    plt.ylim(0, 1000)\n",
    "\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    plt.title(\"Loss function\")\n",
    "    plt.xlabel(\"$beta1$\")\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylabel(\"$J$\")\n",
    "    plt.ylim(0, 1500000)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    ax0.scatter(df[\"flarea\"], df[\"price\"], color = 'green')\n",
    "    xvals = np.linspace(-500, 500, 3)\n",
    "    ax0.plot(xvals, xvals * beta1, color = \"blue\")\n",
    "    ax1.scatter(beta1, J(X, y, beta1), color = \"red\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot = interactive(show_linear_model_and_loss, beta1=(-10,10,.1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Another 2D Visualization of $J$</h2>\n",
    "<ul>\n",
    "    <li>Instead of making manual adjustments, let's use a loop to try several values for $\\v{\\beta}_1$</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4)) \n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
    "ax0 = plt.subplot(gs[0])\n",
    "plt.title(\"Training set\")\n",
    "plt.xlabel(\"Floor area (sq metres)\")\n",
    "plt.xlim(-500, 500)\n",
    "plt.ylabel(\"Price (000 euros)\")\n",
    "plt.ylim(0, 1000)\n",
    "ax1 = plt.subplot(gs[1])\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"$beta1$\")\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(0, 1500000)\n",
    "fig.tight_layout()\n",
    "ax0.scatter(df[\"flarea\"], df[\"price\"], color = 'green')\n",
    "\n",
    "xvals = np.linspace(-500, 500, 3)\n",
    "beta1s = np.linspace(-10, 10, 21)\n",
    "\n",
    "for beta1 in beta1s:\n",
    "    ax0.plot(xvals, xvals * beta1, color = \"blue\")\n",
    "\n",
    "ax1.scatter(beta1s, [J(X, y, beta1) for beta1 in beta1s], color = \"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The loss function is <b>convex</b>.</li>\n",
    "    <li>Informally, this means:\n",
    "        <ul>\n",
    "            <li>\n",
    "                in 2D, it is u-shaped;\n",
    "            </li>\n",
    "            <li>\n",
    "                it has a unique minimum.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This is no accident: it follows from the way the loss function has been defined.</li>\n",
    "    <li>(To be 100% accurate: there is a unique minimum provided no feature in $\\v{X}$ is linearly dependent on any of the others or, equivalently, provided $\\v{X}$ has an inverse.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>3D Visualization of $J$</h2>\n",
    "<ul>\n",
    "    <li>Let's visualize $J$ again using the Cork Property Prices Dataset as the training set.</li>\n",
    "    <li>This time, we'll assume that $\\v{\\beta}_j = 0$ for all $j$ except $j = 2$ and $j = 3$.\n",
    "        <ul>\n",
    "            <li>In other words, we are pretending that the number of bedrooms and bathrooms are the only relevant \n",
    "               features.\n",
    "            </li>\n",
    "        </ul>\n",
    "        This will be a 3D plot with $J$ on the vertical axis against different values of $\\v{\\beta}_2$ \n",
    "        and $\\v{\\beta}_3$ on the horizontal axes.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_title(\"Loss function\")\n",
    "ax.set_xlabel(\"$beta_2$\")\n",
    "ax.set_ylabel(\"$beta_3$\")\n",
    "ax.set_zlabel(\"$J$\")\n",
    "xvals = np.linspace(-100, 200, 301)\n",
    "yvals = np.linspace(-100, 200, 301)\n",
    "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
    "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
    "zvals = zs.reshape(xxvals.shape)\n",
    "ax.plot_surface(xxvals, yyvals, zvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Another Visualization of $J$</h2>\n",
    "<ul>\n",
    "    <li>Here is the same data that we had in 3D but on a different kind of plot, called a <b>contour plot</b>.</li>\n",
    "    <li>In effect, it flattens the diagram above.</li>\n",
    "    <li>The lines connect points that have the same value for $J$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6, 6)) \n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"$beta_2$\")\n",
    "plt.ylabel(\"$beta_3$\")\n",
    "xvals = np.linspace(-100, 200, 301)\n",
    "yvals = np.linspace(-100, 200, 301)\n",
    "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
    "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
    "zvals = zs.reshape(xxvals.shape)\n",
    "C = plt.contour(xxvals, yyvals, zvals, 15, colors = \"black\")\n",
    "plt.clabel(C, inline=1, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The 3D visualization and the contour plot show that this too is convex.</li>\n",
    "    <li>Informally, this means:\n",
    "        <ul>\n",
    "            <li>\n",
    "                in 3D it is bowl-shaped;\n",
    "            </li>\n",
    "            <li>\n",
    "                again there is a unique minimum.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Ordinary Least Squares Linear Regression</h1>\n",
    "<ul>\n",
    "    <li>How do we find the best values for $\\v{\\beta}$?</li>\n",
    "    <li>There are two methods:\n",
    "        <ul>\n",
    "            <li>The Normal Equation; and</li>\n",
    "            <li>Gradient Descent.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>The Normal Equation</h1>\n",
    "<ul>\n",
    "    <li>The <b>normal equation</b> solves for $\\v{\\beta}$:\n",
    "        $$\\v{\\beta} = (\\v{X}^T\\v{X})^{-1}\\v{X}^T\\v{y}$$\n",
    "        <ul>\n",
    "            <li>In other words, the normal equation gives us the\n",
    "                parameters that minimize the loss function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Where does it come from?\n",
    "        <ul>\n",
    "            <li>Take the gradient of the loss function: $\\frac{1}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$ (see next slide).</li>\n",
    "            <li>Set it to zero: $\\frac{1}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y}) = \\v{0}$ (in fact, a\n",
    "                $(n+1)$-dimensional vector of zeros).\n",
    "            </li>\n",
    "            <li>Then do some algebraic manipulation to get $\\v{\\beta}$ on the left-hand side:\n",
    "                $\\v{\\beta} = (\\v{X}^T\\v{X})^{-1}\\v{X}^T\\v{y}$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Partial Derivatives</h1>\n",
    "<ul>\n",
    "    <li>We need the <b>gradient</b> of the loss function with regards to each $\\v{\\beta}_j$.\n",
    "        <ul>\n",
    "            <li>In other words, how much the loss will change if we change $\\v{\\beta}_j$ a little.</li>\n",
    "            <li>With respect to a particular $\\v{\\beta}_j$, it is called the <b>partial derivative</b>.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Without doing the calculus, the partial derivatives of $J(\\v{X}, \\v{y}, \\v{\\beta})$ with respect to\n",
    "        $\\v{\\beta}_j$ are\n",
    "        $$\\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_j} = \n",
    "          \\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)}\\v{\\beta}_j - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
    "    </li>\n",
    "    <li>The <b>gradient vector</b>, $\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta})$, is a vector\n",
    "        of each partial derivative:\n",
    "        $$\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta}) = \n",
    "            \\cv{\\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_0}\\\\ \n",
    "              \\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_1}\\\\ \n",
    "              \\vdots\\\\ \n",
    "              \\frac{\\partial J(\\v{X}, \\v{y}, \\v{\\beta})}{\\partial\\v{\\beta}_n}} =\n",
    "              \\cv{\\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)}\\v{\\beta}_0 - \\v{y}^{(i)}) \\times \\v{x}_0^{(i)}\\\\\n",
    "                \\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)}\\v{\\beta}_1 - \\v{y}^{(i)}) \\times \\v{x}_1^{(i)}\\\\\n",
    "                \\vdots\\\\\n",
    "                \\frac{1}{m}\\sum_{i=1}^m(\\v{x}^{(i)}\\v{\\beta}_n - \\v{y}^{(i)}) \\times \\v{x}_n^{(i)}}\n",
    "        $$\n",
    "     </li>\n",
    "     <li>And there is a vectorized way to compute it:\n",
    "         $$\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta}) = \\frac{1}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$$\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Normal Equation in scikit-learn</h1>\n",
    "<ul>\n",
    "    <li>The <code>fit</code> method of scikit-learn's <code>LinearRegression</code> class does\n",
    "        what we have described:\n",
    "        <ul>\n",
    "            <li>It inserts the extra column of 1s.</li>\n",
    "            <li>It calculates $\\v{\\beta}$ using the Normal Equation.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Normal Equation in numpy</h1>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's see how to implement it ourselves.</li>\n",
    "    <li>Again for the purposes of this explanation, we will use the entire dataset as our training set.\n",
    "        <ul>\n",
    "            <li>We will learn later that using <em>all</em> the data for training is usually not\n",
    "                the right thing to do.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X)\n",
    "\n",
    "# Solve the normal equation\n",
    "beta = npla.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Essentially, one line of code!</li>\n",
    "    <li>But there's a problem:\n",
    "        <ul>\n",
    "            <li>The normal equation requires that $\\v{X}^T\\v{X}$ has an inverse.</li>\n",
    "            <li>But it might not (see earlier lecture about matrices).</li>\n",
    "        </ul>\n",
    "        When we discussed this before, we mentioned that, in some cases, we can use the \n",
    "        <em>pseudo-inverse</em> instead.\n",
    "        <ul>\n",
    "            <li>This is one of those cases.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So the more robust way of writing this program is:\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the normal equation - but using the pseudo-inverse\n",
    "beta = npla.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
