{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Datasets</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Features</h1>\n",
    "<ul>\n",
    "    <li>Suppose we want to store data about objects, such as houses.</li>\n",
    "    <li><b>Features</b> describe the houses, e.g.\n",
    "        <ul>\n",
    "            <li>$\\mathit{flarea}$: the total floor area (in square metres);</li>\n",
    "            <li>$\\mathit{bdrms}$: the number of bedrooms;</li>\n",
    "            <li> $\\mathit{bthrms}$: the number of bathrooms.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A particular house has <b>values</b> for the features:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\mathit{flarea} = 126, \\mathit{bdrms} = 3, \\mathit{bthrms} = 1$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we can represent a house using a vector:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\cv{126\\\\3\\\\1}$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will always use $n$ to refer to the number of features, e.g. above $n = 3$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Examples</h1> \n",
    "<ul>\n",
    "    <li>Suppose we collect a <b>dataset</b> containing data about lots of houses, e.g.:\n",
    "        $$\\cv{126\\\\3\\\\1} \\,\\, \\cv{92.9\\\\3\\\\2} \\,\\,\\cv{171.9\\\\4\\\\3} \\,\\, \\cv{79\\\\3\\\\1}$$\n",
    "    </li>\n",
    "    <li>Each member of this dataset is called an <b>example</b>, and we will use $m$ to refer to the number of examples, e.g.\n",
    "        above $m = 4$.\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset notation</h1>\n",
    "<ul>\n",
    "    <li>We will use a <em>superscript</em> to index the examples.\n",
    "        <ul>\n",
    "            <li>\n",
    "                $\\v{x}^{(i)}$ will be the $i$th example.\n",
    "            </li>\n",
    "            <li>\n",
    "                The first example in the dataset is $\\v{x}^{(1)}$, the second is $\\v{x}^{(2)}$, $\\ldots$, \n",
    "                the last is $\\v{x}^{(m)}$ (Note, we index from 1.)\n",
    "            </li>\n",
    "            <li>\n",
    "                We're writing the superscript in parentheses to make it clear that we are using it for indexing.\n",
    "                It is not 'raising to a power'. If we want to raise to a power, we will drop the parentheses.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use a <em>subscript</em> to index the features (again starting from 1).</li>\n",
    "    <li>Class exercise. Using the dataset from above:\n",
    "        <ul>\n",
    "            <li>what is $\\v{x}_2^{(1)}$?</li>\n",
    "            <li>what is $\\v{x}_1^{(2)}$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset as a matrix</h1>\n",
    "<ul>\n",
    "    <li>We can represent a dataset $\\Set{\\v{x}^{(1)}, \\v{x}^{(2)}, \\ldots, \\v{x}^{(m)}}$ as a $m \\times n$\n",
    "        matrix $\\v{X}$ as follows:\n",
    "        $$\\v{X} = \\begin{bmatrix}\n",
    "              \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "              \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "              \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "              \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "              \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "    <li>Note how each example becomes a <em>row</em> in $\\v{X}$.</li>\n",
    "    <li>You can think of row $i$ as the transpose of $\\v{x}^{(i)}$.</li>\n",
    "    <li>For the example dataset, we get:\n",
    "        $$\\v{X} = \n",
    "            \\begin{bmatrix}\n",
    "                126 & 3 & 1 \\\\\n",
    "                92.9 & 3 & 2 \\\\\n",
    "                171.9 & 4 & 3 \\\\\n",
    "                79 & 3 & 1\n",
    "            \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cork Property Prices Dataset</h1>\n",
    "<ul>\n",
    "    <li>In August 2019, I scraped a dataset of property prices for Cork city from www.daft.ie.</li>\n",
    "    <li>They are in a CSV file. Each line in the file is an example, representing one house.</li>\n",
    "    <li>Hence, each line of the file contains the feature-values for the floor area, number of bedrooms, number of\n",
    "        bathrooms, and several other features that we will ignore for now.\n",
    "    </li>\n",
    "    <li>We will use the pandas library:\n",
    "        <ul>\n",
    "            <li>to read the dataset from the csv file into what pandas calls a DataFrame;</li>\n",
    "            <li>to explore the dataset: looking at values and computing summary statistics.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we will extract some of the features (columns) and convert to a numpy 2D array, before using the data\n",
    "        to find houses similar to yours.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using pandas to Read and Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flarea', 'bdrms', 'bthrms', 'price'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    float64\n",
       "bdrms       int64\n",
       "bthrms      int64\n",
       "price       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464 entries, 0 to 463\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   flarea  464 non-null    float64\n",
      " 1   bdrms   464 non-null    int64  \n",
      " 2   bthrms  464 non-null    int64  \n",
      " 3   price   464 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 14.6 KB\n"
     ]
    }
   ],
   "source": [
    "# The columns and datatypes (again) but also whether there are any nulls in the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.460151</td>\n",
       "      <td>3.329741</td>\n",
       "      <td>2.120690</td>\n",
       "      <td>352.297414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.692202</td>\n",
       "      <td>1.068445</td>\n",
       "      <td>1.061033</td>\n",
       "      <td>197.464495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>575.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flarea       bdrms      bthrms        price\n",
       "count  464.000000  464.000000  464.000000   464.000000\n",
       "mean   125.460151    3.329741    2.120690   352.297414\n",
       "std     70.692202    1.068445    1.061033   197.464495\n",
       "min     40.000000    1.000000    1.000000    95.000000\n",
       "25%     82.000000    3.000000    1.000000   235.000000\n",
       "50%    110.000000    3.000000    2.000000   295.000000\n",
       "75%    140.600000    4.000000    3.000000   395.000000\n",
       "max    575.000000    9.000000    6.000000  1495.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea  bdrms  bthrms  price\n",
       "0   111.9      3       3    305\n",
       "1    95.0      3       3    255\n",
       "2   120.8      3       3    275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Convert to a numpy 2D array</h2>\n",
    "<ul>\n",
    "    <li>We will select certain features (columns) from the pandas DataFrame\n",
    "        and convert to a 2D numpy array\n",
    "    </li>\n",
    "    <li>(Later in the module, we will use a <code>ColumnTransformer</code> to do this.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract these features and convert to numpy 2D array\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111.9,   3. ,   3. ],\n",
       "       [ 95. ,   3. ,   3. ],\n",
       "       [120.8,   3. ,   3. ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Similarity &amp; Distance</h1>\n",
    "<ul>\n",
    "    <li>In AI, we often want to know how <em>similar</em> one object is to another.\n",
    "        <ul>\n",
    "            <li>E.g. how similar is my house to yours?</li>\n",
    "            <li>E.g. which house in our dataset is most similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In fact, here we are instead going to measure how <em>different</em> they are using a <b>distance function</b>.\n",
    "        <ul>\n",
    "            <li>(N.B. This is not about geographical distance.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another.</li>\n",
    "    <li>Simplest is to measure their <b>Euclidean distance</b>:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
    "        or, more concisely:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
    "    </li>\n",
    "    <li>Euclidean distance has a minimum value of 0 (meaning identical) but no maximum value (depends on your data).</li>\n",
    "    <li>Class exercise. What is the Euclidean distance between $\\v{x} = \\cv{100\\\\1\\\\4}$ and $\\v{x}' = \\cv{100\\\\5\\\\1}$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Euclidean Distance in numpy</h2>\n",
    "<ul>\n",
    "    <li>It has a nice vectorized implementation (no loop!) using numpy:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.026297590440446"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "your_house = np.array([126.0, 3, 1])\n",
    "my_house = np.array([107.0, 2, 1])\n",
    "\n",
    "euc(your_house, my_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute the distance between your house and all the houses in X.</li>\n",
    "    <li>(We have to write a loop here, because our <code>euc</code> function is not vectorized.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [euc(your_house, x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.241137595009741, 31.064449134018133, 5.571355310873651]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show you, here are the first 3 distances\n",
    "dists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0332473082471605"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, we can, with one line of code, find the most similar house\n",
    "np.min([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better again, we can find which house is the most similar\n",
    "np.argmin([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    125.74\n",
       "bdrms       3.00\n",
       "bthrms      2.00\n",
       "price     398.00\n",
       "Name: 196, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best of all, we can display the most similar house\n",
    "df.iloc[np.argmin([euc(your_house, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Euclidean distance</h1>\n",
    "<ul>\n",
    "    <li>There are at least three problems with Euclidean distance (and many other distance measures too):\n",
    "        <ul>\n",
    "            <li>Features with different scales;</li>\n",
    "            <li>Features that are correlated with each other;</li>\n",
    "            <li>The curse of dimensionality.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Scaling Numeric Values</h1>\n",
    "<ul>\n",
    "    <li>Different numeric-valued features often have very different ranges.\n",
    "        <ul>\n",
    "            <li>E.g. the values for floor area are going to range from a few tens to a few hundreds of square metres.</li>\n",
    "            <li>But the number of bedrooms and bathrooms is going to range from 0 to a dozen or so at most.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When computing the Euclidean distance, features with large ranges will dominate the distance calculations, \n",
    "        thus giving features with small ranges negligible influence.\n",
    "    </li>\n",
    "    <li>\n",
    "        E.g., consider your house $\\v{x} = \\cv{126\\\\3\\\\1}$ and two others, $\\v{y} = \\cv{131\\\\3\\\\1}$ and\n",
    "        $\\v{z} = \\cv{126\\\\7\\\\1}$. \n",
    "        <ul>\n",
    "            <li><em>Intuitively</em>, which house is more similar to yours, $\\v{y}$ or $\\v{z}$?</li>\n",
    "            <li>Now compute the Euclidean distances.</li>\n",
    "            <li>According to these distances, which house is more similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The solution is to <b>scale</b> (or 'normalize') the values so that they have similar ranges.\n",
    "    </li>\n",
    "    <li>There are several ways to do this. One is <b>min-max scaling</b>, but the one we'll discuss is <b>standardization</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization</h2>\n",
    "<ul>\n",
    "    <!--\n",
    "    <li>In some cases, you don't want feature values to have the same range but to have the same mean\n",
    "        and even the same variance\n",
    "    </li>\n",
    "    -->\n",
    "    <li>\n",
    "        One idea is <b>mean centering</b>, where you subtract the mean value of the feature.\n",
    "        <ul>\n",
    "            <li>If you do this to all values, some of the new values will be positive and some will be negative and \n",
    "                their mean will be approximately zero.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
    "        deviation:\n",
    "        $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
    "        where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation\n",
    "    </li>\n",
    "    <li>\n",
    "        If you use this, then the mean will be approximately zero, the standard deviation will be 1.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn provides a class called <code>StandardScaler</code>.\n",
    "    </li>\n",
    "    <li>It uses means and standard deviations that it calculates from your dataset. (Statisticians would say that it should\n",
    "        use the population mean and standard deviation, but these are generally not known.)\n",
    "    </li>\n",
    "    <li>We create the scaler and then run its <code>fit</code> and <code>transform</code> methods.</li>\n",
    "    <li>(Later in the module, when we are using a <code>ColumnTransformer</code>, running these methods\n",
    "        will be done for us.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19202665, -0.30895098,  0.82962481],\n",
       "       [-0.43134924, -0.30895098,  0.82962481],\n",
       "       [-0.06599286, -0.30895098,  0.82962481]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00764486, -0.30895098, -1.05736495])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's scale your house too\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house = np.array([[126.0, 3, 1]])\n",
    "your_house_scaled = scaler.transform(your_house)[0]\n",
    "your_house_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see what effect this has had, let's see which house is most similar to yours\n",
    "np.argmin([euc(your_house_scaled, x) for x in X_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    122.4\n",
       "bdrms       3.0\n",
       "bthrms      1.0\n",
       "price     295.0\n",
       "Name: 328, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argmin([euc(your_house_scaled, x) for x in X_scaled])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Features that are Correlated</h1>\n",
    "<ul>\n",
    "    <li>Let's start with an extreme example. \n",
    "        <ul>\n",
    "            <li>Suppose one feature is the floor area in square metres and\n",
    "                another is the floor area in square feet.\n",
    "                Then it's clear that, even after scaling, when calculating distances, floor area will have greater\n",
    "                influence than other features, such as the number of bedrooms, because it is in the dataset twice.\n",
    "            </li>\n",
    "            <li>Examples are often less stark. For example, floor area and the number of bedrooms are correlated,\n",
    "                and so their contributions to the distance calculations are not independent of each other.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Ideally the features should be independent (at least, linearly independent).</li>\n",
    "    <li>Yet, few people who use distances do anything about this problem!</li>\n",
    "    <li>Solutions (which we're not covering in detail) include feature weighting and projections to\n",
    "        a new feature space whose features are (linearly) independent (e.g. using Principal Component\n",
    "        Analysis).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Curse of Dimensionality</h1>\n",
    "<ul>\n",
    "    <li>In some datasets, examples have thousands or even millions of features.\n",
    "        <ul>\n",
    "            <li>E.g. datasets from astronomy;</li>\n",
    "            <li>E.g. datasets of images and videos;</li>\n",
    "            <li>E.g. datasets of documents where each unique word is a feature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is it better or worse to have more features?\n",
    "        <ul>\n",
    "            <li>Storage and processing costs increase.</li>\n",
    "            <li>Apart from efficiency, intuitively, more features is better:\n",
    "                <ul>\n",
    "                    <li>e.g. describing houses more completely.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But, counter-intuitively, that isn't true in general.\n",
    "                <ul>\n",
    "                    <li>As the number of features grows, algorithms that use distance and density, will find it harder \n",
    "                        to find good solutions.\n",
    "                    </li>\n",
    "                    <li>The problems that arise as the number of features grows have been called <b>the curse of dimensionality</b>.\n",
    "    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of the Curse of Dimensionality</h2>\n",
    "<ul>\n",
    "    <li>The code that follows (which you don't need to study):\n",
    "        <ul>\n",
    "            <li>generates a random dataset where $m = 400$ and $n = 2$ and both features have values in $[0, 1)$;\n",
    "            </li>\n",
    "            <li>computes the Euclidean distance between all pairs of examples;</li>\n",
    "            <li>finds $d_{\\mathit{min}}$, the smallest of these distances;</li>\n",
    "            <li>finds $d_{\\mathit{max}}$, the largest of the distances;</li>\n",
    "            <li>computes the ratio $\\frac{d_{\\mathit{max}}}{d_{\\mathit{min}}}$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It then does this all again but with $n = 3, 4, 5,\\ldots,500$.</li>\n",
    "    <li>Then it plots the ratios that it has computed ($y$-axis, but note its scale) against $n$ ($x$-axis).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 400\n",
    "n_range = range(1, 500)\n",
    "\n",
    "ratios = []\n",
    "for n in n_range:\n",
    "    X = rand(m, n)\n",
    "    dists = euclidean_distances(X)\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    ratios += [np.max(non_zero_dists) / (np.min(non_zero_dists))]\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Ratio of max distance to min distance in datasets with ever more features\")\n",
    "plt.scatter(n_range, ratios)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"n\")\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel(\"ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>As $n \\rightarrow \\infty$, $d_{\\mathit{max}} \\rightarrow d_{\\mathit{min}}$, so their ratio tends to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2631611837588679,\n",
       " 1.2373451386078689,\n",
       " 1.272842643074723,\n",
       " 1.2638487678856976,\n",
       " 1.2496565032779254]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it may not be clear from the graph, we'll show the last 5 of the ratios that it calculated\n",
    "ratios[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We conclude (counter-intutively) that examples become equi-distant!</li>\n",
    "    <li>This obviously undermines methods that depend on finding objects that are similar to each other, as we were\n",
    "        doing earlier &mdash; with more features, the most similar object becomes more arbitrary!\n",
    "    </li>\n",
    "    <li>The problem extends to other distance/similarity measures, e.g. cosine similarity.</li>\n",
    "    <li>Fortunately, there are lots of methods available for reducing dimensionality.\n",
    "        One solution is to retain the principle components found by Principal Component Analysis. This is\n",
    "        interesting because PCA was suggested above as a solution to the problem of correlated features. \n",
    "        It can actually help us solve both problems.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
